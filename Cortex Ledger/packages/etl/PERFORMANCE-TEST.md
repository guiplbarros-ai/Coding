# Performance Testing Guide

Este documento descreve o procedimento completo para validar a performance do pipeline ETL do Cortex Ledger.

## Objetivo

Validar que o sistema consegue processar **10.000 transa√ß√µes em ‚â§ 2 minutos**, conforme especificado nos requisitos de aceita√ß√£o do Agent C.

## Pr√©-requisitos

1. Database configurado e migra√ß√µes aplicadas (Agent A)
2. Conta (`conta_id`) criada no sistema
3. Vari√°veis de ambiente configuradas (`.env` com `SUPABASE_URL` e `SUPABASE_SERVICE_KEY`)
4. Pacotes instalados: `pnpm install`

## Etapa 1: Gerar Arquivo de Teste

O script `generate-large-file.ts` cria arquivos CSV realistas para testes de performance.

```bash
# Gerar arquivo com 10.000 transa√ß√µes (padr√£o)
pnpm --filter @cortex/etl tsx scripts/generate-large-file.ts

# Gerar arquivo customizado
pnpm --filter @cortex/etl tsx scripts/generate-large-file.ts 10000 large-test-10k.csv

# Gerar arquivo maior (50k transa√ß√µes)
pnpm --filter @cortex/etl tsx scripts/generate-large-file.ts 50000 large-test-50k.csv
```

**Caracter√≠sticas do arquivo gerado:**
- Data aleat√≥ria em 2024
- 20 templates diferentes de transa√ß√µes realistas
- Formato Bradesco CSV (separador `;`, valores com v√≠rgula)
- Saldo calculado progressivamente
- Documentos √∫nicos por linha

**Sa√≠da esperada:**
```
üîß Generating CSV file with 10000 transactions...
‚úÖ File created: /path/to/large-test-file.csv
üìä Size: 1024.56 KB
üìù Lines: 10001 (including header)

üí° Usage:
   pnpm --filter @cortex/etl dev large-test-file.csv <conta_id> bradesco-csv
```

## Etapa 2: Executar Teste de Performance

### 2.1 Prepara√ß√£o

```bash
# 1. Certifique-se que o database est√° limpo ou use uma conta de teste
# 2. Anote o conta_id que voc√™ vai usar
CONTA_ID="sua-conta-id-aqui"

# 3. Navegue at√© o diret√≥rio raiz do projeto
cd /path/to/Cortex\ Ledger
```

### 2.2 Executar Import com Medi√ß√£o de Tempo

**Linux/macOS:**
```bash
time pnpm --filter @cortex/etl dev large-test-10k.csv $CONTA_ID bradesco-csv
```

**Windows (PowerShell):**
```powershell
Measure-Command { pnpm --filter @cortex/etl dev large-test-10k.csv $env:CONTA_ID bradesco-csv }
```

### 2.3 Interpretar Resultados

**Sa√≠da esperada do CLI:**
```
üìÇ Importing: large-test-10k.csv
üìã Template: bradesco-csv
üè¶ Account: 123e4567-e89b-12d3-a456-426614174000

‚úÖ Parsing complete
   Transactions: 10000
   Errors: 0
   Skipped: 0

üì§ Uploading to Supabase...
   Batch 1/10: 1000 rows ‚úì
   Batch 2/10: 1000 rows ‚úì
   ...
   Batch 10/10: 1000 rows ‚úì

‚úÖ Import complete!
   Total imported: 10000
   Duplicates skipped: 0
   Duration: 87.5s
```

**Medi√ß√£o de tempo (macOS/Linux):**
```
real    1m27.532s
user    0m2.341s
sys     0m0.523s
```

**Crit√©rios de sucesso:**
- ‚úÖ `real` time ‚â§ 120s (2 minutos)
- ‚úÖ Todas as 10.000 transa√ß√µes importadas
- ‚úÖ Sem erros de parsing
- ‚úÖ Hash de deduplica√ß√£o funcionando (re-import deve marcar todas como duplicadas)

## Etapa 3: Valida√ß√£o de Deduplica√ß√£o

Rode o import **novamente** com o mesmo arquivo para validar deduplica√ß√£o:

```bash
time pnpm --filter @cortex/etl dev large-test-10k.csv $CONTA_ID bradesco-csv
```

**Resultado esperado:**
```
‚úÖ Import complete!
   Total imported: 0
   Duplicates skipped: 10000
   Duration: 45.2s
```

**Crit√©rios de sucesso:**
- ‚úÖ 0 transa√ß√µes importadas
- ‚úÖ 10.000 duplicatas detectadas
- ‚úÖ Tempo menor que primeira execu√ß√£o (sem insert, s√≥ verifica√ß√£o de hash)

## Etapa 4: Valida√ß√£o no Database

Conecte ao Supabase e execute queries de valida√ß√£o:

```sql
-- 1. Verificar total de transa√ß√µes importadas
SELECT COUNT(*) FROM transacoes WHERE conta_id = 'sua-conta-id';
-- Esperado: 10000

-- 2. Verificar integridade dos hashes (sem duplicatas)
SELECT hash_dedupe, COUNT(*)
FROM transacoes
WHERE conta_id = 'sua-conta-id'
GROUP BY hash_dedupe
HAVING COUNT(*) > 1;
-- Esperado: 0 rows (nenhum hash duplicado)

-- 3. Verificar range de datas
SELECT MIN(data), MAX(data)
FROM transacoes
WHERE conta_id = 'sua-conta-id';
-- Esperado: 2024-01-01 to 2024-12-31

-- 4. Verificar somat√≥rio de valores
SELECT
  COUNT(*) as total,
  SUM(CASE WHEN valor > 0 THEN valor ELSE 0 END) as total_creditos,
  SUM(CASE WHEN valor < 0 THEN valor ELSE 0 END) as total_debitos,
  SUM(valor) as saldo_liquido
FROM transacoes
WHERE conta_id = 'sua-conta-id';
```

## Benchmarks de Refer√™ncia

### Hardware de Teste
- **Dev Machine**: MacBook Pro M1, 16GB RAM
- **Network**: Banda larga 100Mbps
- **Supabase**: Free tier (regi√£o us-east-1)

### Resultados Esperados

| Transa√ß√µes | Tamanho | Parse Time | Upload Time | Total Time | Status |
|-----------|---------|------------|-------------|------------|--------|
| 1.000     | ~100 KB | ~2s        | ~8s         | ~10s       | ‚úÖ     |
| 10.000    | ~1 MB   | ~8s        | ~75s        | ~85s       | ‚úÖ     |
| 50.000    | ~5 MB   | ~35s       | ~380s       | ~415s      | ‚ö†Ô∏è     |
| 100.000   | ~10 MB  | ~70s       | ~780s       | ~850s      | ‚ö†Ô∏è     |

**Notas:**
- ‚úÖ = Dentro do SLA (‚â§2min para 10k)
- ‚ö†Ô∏è = Acima do SLA, mas funcional
- Batch size: 1000 rows (otimizado para Supabase)
- Re-import (dedup): ~50% mais r√°pido (sem inserts)

## Troubleshooting

### Performance Abaixo do Esperado

**Sintoma**: Import de 10k leva > 2 minutos

**Poss√≠veis causas:**

1. **Network latency**
   ```bash
   # Testar lat√™ncia para Supabase
   ping your-project.supabase.co
   ```
   - Solu√ß√£o: Usar regi√£o mais pr√≥xima

2. **Batch size muito grande/pequeno**
   - Atual: 1000 rows/batch
   - Ajustar em `packages/etl/src/cli/import.ts:BATCH_SIZE`

3. **RLS muito complexo**
   - Verificar policies no Supabase
   - Temporariamente desabilitar RLS para teste

4. **Falta de √≠ndices**
   ```sql
   -- Verificar √≠ndices existentes
   SELECT indexname, indexdef
   FROM pg_indexes
   WHERE tablename = 'transacoes';
   ```
   - Deve ter √≠ndice em `hash_dedupe` e `conta_id`

### Parsing Lento

**Sintoma**: Parse time > 10s para 10k linhas

**Solu√ß√µes:**
- Verificar se arquivo tem encoding correto (UTF-8)
- Verificar se h√° linhas malformadas (aumenta skipped count)
- Usar template espec√≠fico ao inv√©s de auto-detect

### Erros de Upload

**Sintoma**: Batches falhando durante upload

**Debug:**
```bash
# Executar com logs detalhados
DEBUG=* pnpm --filter @cortex/etl dev large-test-10k.csv $CONTA_ID bradesco-csv
```

**Verificar:**
- Service key tem permiss√µes corretas
- RLS policies permitem insert
- Foreign key para `conta_id` existe

## Automa√ß√£o (Opcional)

### Script Bash Completo

Crie `scripts/performance-test.sh`:

```bash
#!/bin/bash
set -e

echo "üöÄ Cortex Ledger - Performance Test"
echo "===================================="

# Config
CONTA_ID="${1:-}"
NUM_LINES="${2:-10000}"
TEST_FILE="perf-test-${NUM_LINES}.csv"

if [ -z "$CONTA_ID" ]; then
  echo "‚ùå Erro: conta_id n√£o fornecido"
  echo "Uso: ./scripts/performance-test.sh <conta_id> [num_lines]"
  exit 1
fi

# 1. Gerar arquivo
echo ""
echo "üìù Gerando arquivo de teste com ${NUM_LINES} linhas..."
pnpm --filter @cortex/etl tsx scripts/generate-large-file.ts "$NUM_LINES" "$TEST_FILE"

# 2. Executar import
echo ""
echo "‚è±Ô∏è  Executando import (medindo tempo)..."
time pnpm --filter @cortex/etl dev "$TEST_FILE" "$CONTA_ID" bradesco-csv

# 3. Testar deduplica√ß√£o
echo ""
echo "üîÑ Testando deduplica√ß√£o (re-import)..."
time pnpm --filter @cortex/etl dev "$TEST_FILE" "$CONTA_ID" bradesco-csv

# 4. Cleanup
echo ""
echo "üßπ Limpando arquivo de teste..."
rm "$TEST_FILE"

echo ""
echo "‚úÖ Performance test completo!"
```

**Uso:**
```bash
chmod +x scripts/performance-test.sh
./scripts/performance-test.sh <sua-conta-id> 10000
```

## M√©tricas de Sucesso

Checklist para valida√ß√£o completa:

- [ ] ‚úÖ 10k transa√ß√µes em ‚â§ 2min
- [ ] ‚úÖ 0 erros de parsing
- [ ] ‚úÖ 0 duplicatas no primeiro import
- [ ] ‚úÖ 100% duplicatas detectadas no re-import
- [ ] ‚úÖ Todos os hashes √∫nicos no database
- [ ] ‚úÖ Dados corretos (datas, valores, descri√ß√µes)
- [ ] ‚úÖ RLS funcionando (apenas dono v√™ transa√ß√µes)

## Pr√≥ximos Passos

Ap√≥s validar performance:

1. **Agent C**: ‚úÖ Marcar performance test como completo
2. **Agent A**: Validar que √≠ndices est√£o otimizados
3. **Agent B**: Implementar dashboard de m√©tricas de import
4. **DevOps**: Configurar monitoramento de performance em produ√ß√£o

---

**Documenta√ß√£o atualizada:** 2025-01-26
**Respons√°vel:** Agent C (AGENT_IMPORT_ETL)
